{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a7b08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "509f8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_DP():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # actions are list or array. States, V, R and policy are dictionaries with keys being the states\n",
    "        self.actions = actions \n",
    "        self.states = states\n",
    "        self.V = {s:0 for s in self.states} #dictionary\n",
    "        self.R = R \n",
    "        self.policy = policy\n",
    "        # next_state is a funcction that takes in initial state and action as tuple to return next state\n",
    "        self.next_state = next_state \n",
    "        # gamma is the discount rate\n",
    "        self.gamma = gamma\n",
    "        # the states in which there shouldn't be any policy improvement\n",
    "        self.terminal_states = terminal_states\n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self):\n",
    "        \n",
    "        # function that loops through all states and applies bellman equation to calculate their value\n",
    "        # delta is a small number used to test for convergence of the value of states\n",
    "        \n",
    "        delta = 0.00000001\n",
    "        d = 1000\n",
    "        \n",
    "        while d > delta:\n",
    "            \n",
    "            V_new = self.V.copy()\n",
    "            for i in self.V:\n",
    "                v = 0\n",
    "                for action in self.actions:\n",
    "                    j = self.next_state(i, action)\n",
    "                    v += self.policy[i][action]*(self.R[i] + self.gamma*self.V[j])                \n",
    "                V_new[i] = v\n",
    "            \n",
    "            d = self.get_delta(self.V,V_new)\n",
    "            self.V = V_new\n",
    "            \n",
    "                   \n",
    "    def get_delta(self,v,vnew):\n",
    "        \n",
    "        # function that returns the absolute value of the mean difference between previous and current values of a state\n",
    "        d = 0\n",
    "        for i in v:\n",
    "            d += abs(v[i]-vnew[i])/len(self.V)\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    def policy_improvement(self):\n",
    "        \n",
    "        # function that changes the policy in accordance to the value of the next states\n",
    "        stable = False\n",
    "        while stable == False:\n",
    "            stable = True\n",
    "            for i in self.policy:\n",
    "                new_policy = np.zeros(len(self.actions))\n",
    "                \n",
    "                pi_s = []\n",
    "                for action in self.actions: \n",
    "                    j = self.next_state(i, action)\n",
    "                    pi_s.append(self.V[j])\n",
    "                    \n",
    "                new_policy = self.get_new_policy(pi_s,i)\n",
    "\n",
    "                if not np.all(new_policy == self.policy[i]):\n",
    "                    stable = False\n",
    "                    self.policy[i] = new_policy\n",
    "                    \n",
    "            if not stable:\n",
    "                self.policy_evaluation()\n",
    "                \n",
    "    def get_new_policy(self, val, state):\n",
    "        \n",
    "        #function that takes in values of states to return the ones with highest value\n",
    "        if state in self.terminal_states:\n",
    "            new_policy = np.zeros(4)\n",
    "        else:\n",
    "            maximum = max(val)\n",
    "            indeces = []\n",
    "            for i in range(len(val)):\n",
    "                if val[i] == maximum:\n",
    "                    indeces.append(i)\n",
    "            new_policy = np.zeros(len(val))\n",
    "            new_policy[indeces] = 1\n",
    "            new_policy = np.array([x/new_policy.sum() for x in new_policy])\n",
    "        \n",
    "        return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e5e4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid(MDP_DP):\n",
    "    def __init__(self, shape):\n",
    "        \n",
    "        self.shape = shape #tuple\n",
    "        self.actions = [x for x in range(4)]\n",
    "        self.R = np.ones((shape))*-1\n",
    "        self.R[0,0], self.R[shape[0]-1,shape[1]-1]  = 0, 0\n",
    "        #self.V = np.zeros(shape)\n",
    "        \n",
    "    \n",
    "    def get_v(self):\n",
    "        v={}\n",
    "        for i in range(self.shape[0]):\n",
    "            for j in range(self.shape[1]):\n",
    "                v[(i,j)] = 0\n",
    "#         v[0,0] = 0\n",
    "#         v[self.shape[0]-1, self.shape[1]-1] = 0\n",
    "        \n",
    "        self.V = v\n",
    "    \n",
    "    def get_policy(self):\n",
    "        policy = {}\n",
    "        for i in self.V:\n",
    "            policy[i] = np.ones(len(self.actions))*(1/len(self.actions))\n",
    "        policy[(0,0)] = np.zeros(len(self.actions))\n",
    "        policy[(self.shape[0]-1,self.shape[0]-1)] = np.zeros(len(self.actions))\n",
    "        \n",
    "        self.policy = policy\n",
    "        \n",
    "    def get_reward(self):\n",
    "        r= {}\n",
    "        for i in range(self.shape[0]):\n",
    "            for j in range(self.shape[1]):\n",
    "                r[(i,j)] = -1\n",
    "        r[0,0] = 0\n",
    "        r[self.shape[0]-1, self.shape[1]-1] = 0\n",
    "        \n",
    "        self.R = r\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def next_state(self, x, a):\n",
    "        actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "        move = actions[a]\n",
    "        x_new, y_new = x[0] + move[0], x[1] + move[1]\n",
    "        if x_new == self.shape[0]:\n",
    "            x_new = x[0]\n",
    "        if x_new < 0:\n",
    "            x_new = x[0]\n",
    "        if y_new == self.shape[1]:\n",
    "            y_new = x[1]\n",
    "        if y_new < 0:\n",
    "            y_new = x[1]\n",
    "    \n",
    "        return (x_new, y_new) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2fc9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grid = Grid((4,4))\n",
    "my_grid.get_v()\n",
    "my_grid.get_policy()\n",
    "my_grid.get_reward()\n",
    "my_grid.gamma = 1\n",
    "my_grid.terminal_states = [(0,0),(3,3)]\n",
    "my_grid.policy_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9a64175",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grid.policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "020f82b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.0,\n",
       " (0, 1): -1.0,\n",
       " (0, 2): -2.0,\n",
       " (0, 3): -3.0,\n",
       " (1, 0): -1.0,\n",
       " (1, 1): -2.0,\n",
       " (1, 2): -3.0,\n",
       " (1, 3): -2.0,\n",
       " (2, 0): -2.0,\n",
       " (2, 1): -3.0,\n",
       " (2, 2): -2.0,\n",
       " (2, 3): -1.0,\n",
       " (3, 0): -3.0,\n",
       " (3, 1): -2.0,\n",
       " (3, 2): -1.0,\n",
       " (3, 3): 0.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_grid.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e58eb42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): array([0., 0., 0., 0.]),\n",
       " (0, 1): array([0., 0., 0., 1.]),\n",
       " (0, 2): array([0., 0., 0., 1.]),\n",
       " (0, 3): array([0.5, 0. , 0. , 0.5]),\n",
       " (1, 0): array([0., 1., 0., 0.]),\n",
       " (1, 1): array([0. , 0.5, 0. , 0.5]),\n",
       " (1, 2): array([0.25, 0.25, 0.25, 0.25]),\n",
       " (1, 3): array([1., 0., 0., 0.]),\n",
       " (2, 0): array([0., 1., 0., 0.]),\n",
       " (2, 1): array([0.25, 0.25, 0.25, 0.25]),\n",
       " (2, 2): array([0.5, 0. , 0.5, 0. ]),\n",
       " (2, 3): array([1., 0., 0., 0.]),\n",
       " (3, 0): array([0. , 0.5, 0.5, 0. ]),\n",
       " (3, 1): array([0., 0., 1., 0.]),\n",
       " (3, 2): array([0., 0., 1., 0.]),\n",
       " (3, 3): array([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_grid.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9366377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
